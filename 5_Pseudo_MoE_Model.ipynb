{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4a236ca8-9617-4bdf-8399-d797b71bdf52",
          "showTitle": false,
          "title": ""
        },
        "id": "egsaYSSC9jrJ"
      },
      "source": [
        "\n",
        "# Mixutre-of-Experts - Achieve Massively Scaled, but Efficient, LLM Peformance\n",
        " explore how to build  own, simplified version of a mixture-of-experts (MoE) LLM system. While this method often involves a complex training and transformer configuration,  some of the benefits of this approach in a pseudo-MoE that  will build with some open source LLMs.\n",
        "\n",
        "\n",
        "1. Create own MoE system using open source LLMs\n",
        "1. Build different gating mechanisms to direct different prompts to appropriate \"expert models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "148496ad-4ff4-471c-b26e-afa5fdfe885a",
          "showTitle": false,
          "title": ""
        },
        "id": "AAVtUdy29jrM"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "b5cce02a-d2cd-4318-8c2b-66c58b3c2af4",
          "showTitle": false,
          "title": ""
        },
        "id": "dOWeq-Ja9jrM"
      },
      "source": [
        "#  The Pseudo MoE Model\n",
        " implement a simplified version of an MoE model. Instead of training the experts and gating function together,  use pre-trained transformer models as our experts and a simple rule-based function as our gating function.\n",
        "\n",
        " look at different types of gating mechanisms - hard gating, soft gating, and top-k gating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7ee94e54-c55c-48ea-9f1b-70378ad04056",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "4dcc4a4d204b49f4a6ddda10cefe5764",
            "d5da1176f6c841da931fc468798d3f77"
          ]
        },
        "id": "WawptJO39jrN",
        "outputId": "cb3feb97-af64-4729-ceaa-85163f831f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dcc4a4d204b49f4a6ddda10cefe5764",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/databricks/python/lib/python3.10/site-packages/huggingface_hub/file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw/models/models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5da1176f6c841da931fc468798d3f77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/databricks/python/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "# transformers is a state-of-the-art library for Natural Language Processing tasks, providing a wide range of pre-trained models\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForSequenceClassification, BertTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
        "# torch.nn.functional provides functions that don't have any parameters, such as activation functions, loss functions etc.\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the GPT2 model and tokenizer\n",
        "# GPT2 is an autoregressive language model that uses transformer blocks and byte-pair encoding\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")\n",
        "# The tokenizer is responsible for turning input data into the format that the model expects\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "# BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing pre-training\n",
        "bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
        "# The tokenizer is responsible for turning input data into the format that the model expects\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
        "\n",
        "# Load the T5 model and tokenizer\n",
        "# T5 (Text-to-Text Transfer Transformer) is a transformer model which treats every NLP problem as a text generation task\n",
        "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\", cache_dir=DA.paths.datasets+\"/models\")\n",
        "# The tokenizer is responsible for turning input data into the format that the model expects\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", cache_dir=DA.paths.datasets+\"/models\"+\"/models\")\n",
        "\n",
        "# Define the \"hard gating\" function\n",
        "# This function decides which model to use based on the length of the input\n",
        "def hard_gating_function(input):\n",
        "    if len(input) < 10:\n",
        "        # For inputs less than 10 characters long, use the GPT2 model\n",
        "        return \"gpt2\", gpt2, gpt2_tokenizer\n",
        "    elif len(input) < 100:\n",
        "        # For inputs less than 100 characters long but greater than 10 characters, use the T5 model\n",
        "        return \"t5\" , t5, t5_tokenizer\n",
        "    else:\n",
        "        # For inputs greater than 100 characters, use the BERT model\n",
        "        return \"bert\", bert, bert_tokenizer\n",
        "\n",
        "# Define the \"soft gating\" function\n",
        "# This function assigns a weight to each model based on the length of the input, and all models are used to a certain extent to generate the output\n",
        "def soft_gating_function(input):\n",
        "    # The weights for each model are calculated using the softmax function, which outputs a probability distribution\n",
        "    weights = F.softmax(torch.tensor([len(input), 100 - len(input), len(input)], dtype=torch.float), dim=0)\n",
        "    # The weights for each model are returned along with the models and their tokenizers\n",
        "    return {\"gpt2\": (gpt2, gpt2_tokenizer, weights[0]),\n",
        "            \"bert\": (bert, bert_tokenizer, weights[1]),\n",
        "            \"t5\": (t5, t5_tokenizer, weights[2])}\n",
        "\n",
        "# Define the pseudo MoE model\n",
        "# This function uses the gating function to decide which model(s) to use for a given input\n",
        "def pseudo_moe_model(input, gating_function):\n",
        "    if gating_function == \"hard\":\n",
        "        # If the hard gating function is used, only one model is used for a given input\n",
        "        model_name, model, tokenizer = hard_gating_function(input)\n",
        "        inputs = tokenizer(input, return_tensors=\"pt\")\n",
        "        if model_name == \"t5\":\n",
        "            # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
        "            decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
        "            outputs = model(**inputs, decoder_input_ids=decoder_inputs)\n",
        "        else:\n",
        "            outputs = model(**inputs)\n",
        "        # The output of the model is decoded into a string\n",
        "        decoded_output = tokenizer.decode(outputs.logits[0].argmax(-1).tolist())\n",
        "        # The name of the model used and the decoded output are returned\n",
        "        return model_name, decoded_output\n",
        "    else:  # soft gating\n",
        "        # If the soft gating function is used, all models are used to a certain extent to generate the output\n",
        "        models = soft_gating_function(input)\n",
        "        outputs = []\n",
        "        for model_name, (model, tokenizer, weight) in models.items():\n",
        "            inputs = tokenizer(input, return_tensors=\"pt\")\n",
        "            if model_name == \"t5\":\n",
        "                # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
        "                decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
        "                output = model(**inputs, decoder_input_ids=decoder_inputs)\n",
        "            else:\n",
        "                output = model(**inputs)\n",
        "            # The output of each model is multiplied by its weight\n",
        "            outputs.append((model_name, output.logits * weight))\n",
        "        # The outputs of all models are added together to generate the final output\n",
        "        decoded_outputs = [(model_name, tokenizer.decode(output[0].argmax(-1).tolist())) for model_name, output in outputs]\n",
        "        # The decoded outputs are returned\n",
        "        return decoded_outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7285e846-e594-4665-911d-960d49fcc880",
          "showTitle": false,
          "title": ""
        },
        "id": "dAbPF_-09jrN",
        "outputId": "f18a758c-93dd-4603-d8af-b9e219481133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard gating output: ('t5', '<extra_id_0>This')\nSoft gating output: [('gpt2', '—ation says reported After și readingatoration anyone each of given dem off usinguk After E valuable of ou Aftereaza of of și'), ('bert', '<pad>'), ('t5', '<extra_id_0>We')]\n"
          ]
        }
      ],
      "source": [
        "# Test the hard gating function\n",
        "example_1 = \"Translate to german: This is a short input.\"\n",
        "output = pseudo_moe_model(example_1, gating_function=\"hard\")\n",
        "print(\"Hard gating output:\", output)\n",
        "\n",
        "# Test the soft gating function\n",
        "example_2 = \"This is a longer input. We're adding more text here to make sure it's longer than 50 characters but shorter than 100 characters.\"\n",
        "output = pseudo_moe_model(example_2, gating_function=\"soft\")\n",
        "print(\"Soft gating output:\", output)\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 03L - Deployment of LLMs Lab",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}