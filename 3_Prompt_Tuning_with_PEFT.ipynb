{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "6fba2d42-ed99-4a03-8033-d479ce24d5dd",
          "showTitle": false,
          "title": ""
        },
        "id": "DngtNJxnFsTm"
      },
      "source": [
        "# Prompt Tuning\n",
        "apply prompt tuning to  model of choice using [Parameter-Efficient Fine-Tuning (PEFT) library developed by HuggingFace](https://huggingface.co/docs/peft/index). This PEFT library supports multiple methods to reduce the number of parameters for fine-tuning, including prompt tuning and LoRA.\n",
        "\n",
        "\n",
        "\n",
        "1. Apply prompt tuning to model of choice\n",
        "1. Fine-tune on provided dataset\n",
        "1. Save and share model to HuggingFace hub\n",
        "1. Conduct inference using the fine-tuned model\n",
        "1. Compare outputs from randomly- and text-initialized fine-tuned model vs. foundation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
          "showTitle": false,
          "title": ""
        },
        "id": "9MFTKkWEFsTm",
        "outputId": "6c09b310-6942-4b91-8dff-398e959a0a66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\nCollecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 2.2 MB/s eta 0:00:00\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (1.21.5)\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (4.29.2)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\nCollecting safetensors\n  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 9.1 MB/s eta 0:00:00\nRequirement already satisfied: torch>=1.13.0 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (1.13.1+cpu)\nRequirement already satisfied: accelerate in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (0.19.0)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (5.9.0)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.3.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.28.1)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (3.6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.15.1)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2022.7.9)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.64.1)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2022.7.1)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2022.9.14)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2.0.4)\nInstalling collected packages: safetensors, peft\nSuccessfully installed peft-0.4.0 safetensors-0.3.2\n\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%pip install peft==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "a10a0b182b8c431da7b75779d83d4566",
            "2d050de261a746e5ba6f804f3ac9771d",
            "c51629a277144b588a66a09f0f970a10",
            "b59c7ed2043a416dbd5eafc9cddbdd79",
            "6fa5a7f3fba0420bb3808c38a55ff6ba"
          ]
        },
        "id": "w1ONngduFsTo",
        "outputId": "fbbc7c7b-6ce9-4801-9330-184118ddc3e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a10a0b182b8c431da7b75779d83d4566",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d050de261a746e5ba6f804f3ac9771d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c51629a277144b588a66a09f0f970a10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b59c7ed2043a416dbd5eafc9cddbdd79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fa5a7f3fba0420bb3808c38a55ff6ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
          "showTitle": false,
          "title": ""
        },
        "id": "7JzBI_24FsTo"
      },
      "source": [
        "Before doing any fine-tuning, we will ask the model to generate a new phrase to the following input sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1d4c80a9-4edd-4fcd-aef0-996f4da5cc02",
          "showTitle": false,
          "title": ""
        },
        "id": "voys3ss8FsTo",
        "outputId": "ea4c7df9-d8af-46e8-e85d-43d39c641a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Two things are infinite:  the number of people and the number of things']\n"
          ]
        }
      ],
      "source": [
        "input1 = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
        "foundation_outputs = foundation_model.generate(\n",
        "    input_ids=input1[\"input_ids\"],\n",
        "    attention_mask=input1[\"attention_mask\"],\n",
        "    max_new_tokens=20,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "print(tokenizer.batch_decode(foundation_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
          "showTitle": false,
          "title": ""
        },
        "id": "LQlW_wHmFsTp"
      },
      "source": [
        "The output is not too bad. However, the dataset BLOOMZ is pre-trained on doesn't cover anything about inspirational English quotes. Therefore, we are going to fine-tune `bloomz-560m` on [a dataset called `Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes)  containing exclusively inspirational English quotes, with the hopes of using the fine-tuned version to generate more quotes later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "referenced_widgets": [
            "6f55384bfcc94dee9ba04ab4f81cb75a",
            "eeb30db93a5c4a7a804f1af7e77fedf4"
          ]
        },
        "id": "0ma5WJ86FsTp",
        "outputId": "f0ab7b6f-e0ae-4bc3-fde3-5d770a8aa8fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f55384bfcc94dee9ba04ab4f81cb75a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/5.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found cached dataset json (/dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eeb30db93a5c4a7a804f1af7e77fedf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-768c685e1cb83483.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"Abirate/english_quotes\", cache_dir=DA.paths.datasets+\"/datasets\")\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
        "train_sample = data[\"train\"].select(range(50))\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
          "showTitle": false,
          "title": ""
        },
        "id": "Enr0Mm3XFsTp",
        "outputId": "fc767066-5cc0-4fc4-9fa0-8aa75862a07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\nNone\n"
          ]
        }
      ],
      "source": [
        "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
        "\n",
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
        "    num_virtual_tokens=4,\n",
        "    tokenizer_name_or_path=model_name\n",
        ")\n",
        "peft_model = get_peft_model(foundation_model, peft_config)\n",
        "print(peft_model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
          "showTitle": false,
          "title": ""
        },
        "id": "XDgodKNVFsTq"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "output_directory = os.path.join(DA.paths.working_dir, \"peft_outputs\")\n",
        "\n",
        "if not os.path.exists(DA.paths.working_dir):\n",
        "    os.mkdir(DA.paths.working_dir)\n",
        "if not os.path.exists(output_directory):\n",
        "    os.mkdir(output_directory)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory, # Where the model predictions and checkpoints will be written\n",
        "    no_cuda=True, # This is necessary for CPU clusters.\n",
        "    auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
        "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning\n",
        "    num_train_epochs=1 # Number of passes to go through the entire fine-tuning dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
          "showTitle": false,
          "title": ""
        },
        "id": "WAdBdWRIFsTq",
        "outputId": "f256e6a9-574d-4ec3-c44f-f4d264ec6e2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 02:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7, training_loss=3.6653319767543246, metrics={'train_runtime': 143.4858, 'train_samples_per_second': 0.348, 'train_steps_per_second': 0.049, 'total_flos': 12033285439488.0, 'train_loss': 3.6653319767543246, 'epoch': 1.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
          "showTitle": false,
          "title": ""
        },
        "id": "LLiAtX0aFsTq"
      },
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
          "showTitle": false,
          "title": ""
        },
        "id": "waDLAZNRFsTq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "time_now = time.time()\n",
        "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
        "trainer.model.save_pretrained(peft_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
          "showTitle": false,
          "title": ""
        },
        "id": "vupg2SXtFsTr"
      },
      "source": [
        "## Inference\n",
        "\n",
        "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
          "showTitle": false,
          "title": ""
        },
        "id": "YV3qA-QeFsTr"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "loaded_model = PeftModel.from_pretrained(foundation_model,\n",
        "                                         peft_model_path,\n",
        "                                         is_trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6b44524b-2ac5-4e74-81e6-c406d4414e42",
          "showTitle": false,
          "title": ""
        },
        "id": "Z_uB7nXYFsTr",
        "outputId": "71495a47-92c8-4012-a33f-a5fa5a20bfc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Two things are infinite:  the number of people and the number']\n"
          ]
        }
      ],
      "source": [
        "loaded_model_outputs = loaded_model.generate(\n",
        "    input_ids=input1[\"input_ids\"],\n",
        "    attention_mask=input1[\"attention_mask\"],\n",
        "    max_new_tokens=7,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "print(tokenizer.batch_decode(loaded_model_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "0419dd5c-ccf8-4aa6-8b1d-891905abcd29",
          "showTitle": false,
          "title": ""
        },
        "id": "i9oxIVy0FsTr"
      },
      "source": [
        "## Text initialization\n",
        "\n",
        "Our fine-tuned, randomly initialized model did pretty well on the quote above. Let's now compare it with the text initialization method.\n",
        "\n",
        "Notice that all we are changing is the `prompt_tuning_init` setting and we are also providing a concise text prompt.\n",
        "\n",
        "API docs\n",
        "* [prompt_tuning_init_text](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig.prompt_tuning_init_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ed3c99b1-c953-4e62-b36a-eba5522de00f",
          "showTitle": false,
          "title": ""
        },
        "id": "eAd0SlXoFsTr",
        "outputId": "53409b60-3829-4d70-f2e6-8742834d9310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,072 || all params: 559,217,664 || trainable%: 0.0005493388706691496\nNone\n"
          ]
        }
      ],
      "source": [
        "text_peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    prompt_tuning_init_text=\"Generate inspirational quotes\", # this provides a starter for the model to start searching for the best embeddings\n",
        "    num_virtual_tokens=3, # this doesn't have to match the length of the text above\n",
        "    tokenizer_name_or_path=model_name\n",
        ")\n",
        "text_peft_model = get_peft_model(foundation_model, text_peft_config)\n",
        "print(text_peft_model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5c4ef5bc-da77-4f20-879e-6e9aa3fd94e1",
          "showTitle": false,
          "title": ""
        },
        "id": "1Xl4FEeAFsTs",
        "outputId": "b40d0c6e-b5b6-433d-ccd7-6638e70bc48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 02:07, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7, training_loss=3.153122220720564, metrics={'train_runtime': 134.5159, 'train_samples_per_second': 0.372, 'train_steps_per_second': 0.052, 'total_flos': 12033285439488.0, 'train_loss': 3.153122220720564, 'epoch': 1.0})"
            ]
          },
          "execution_count": 27,
          "metadata": {}
        }
      ],
      "source": [
        "text_trainer = Trainer(\n",
        "    model=text_peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sample,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "text_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0d24f798-1327-48f0-9035-10e2647d5c5d",
          "showTitle": false,
          "title": ""
        },
        "id": "sLH79sjXFsTs",
        "outputId": "c66caf0f-977c-4091-836d-12941f458b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Two things are infinite:  the number of people who will hear']\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "time_now = time.time()\n",
        "text_peft_model_path = os.path.join(output_directory, f\"text_peft_model_{time_now}\")\n",
        "text_trainer.model.save_pretrained(text_peft_model_path)\n",
        "\n",
        "# Load model\n",
        "loaded_text_model = PeftModel.from_pretrained(\n",
        "    foundation_model,\n",
        "    text_peft_model_path,\n",
        "    is_trainable=False\n",
        ")\n",
        "\n",
        "# Generate output\n",
        "text_outputs = text_peft_model.generate(\n",
        "    input_ids=input1[\"input_ids\"],\n",
        "    attention_mask=input1[\"attention_mask\"],\n",
        "    max_new_tokens=7,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(tokenizer.batch_decode(text_outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "feeb20a7-dfaf-4708-9e49-55e7cdcc9359",
          "showTitle": false,
          "title": ""
        },
        "id": "W_8xvOh3FsTs"
      },
      "source": [
        "You can see that text initialization doesn't necessarily perform better than random initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bf31e500-c362-4627-b29f-3d58e79b9a79",
          "showTitle": false,
          "title": ""
        },
        "id": "w71gQVIDFsTs",
        "outputId": "05846bb1-d773-4d9d-a525-62adc205f24d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
              "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)\n",
              "File \u001b[0;32m<command-1738672828157799>:3\u001b[0m\n",
              "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n",
              "\u001b[0;32m----> 3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dbutils\u001b[38;5;241m.\u001b[39msecrets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_scope\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
              "\u001b[1;32m      4\u001b[0m hf_token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
              "\u001b[1;32m      5\u001b[0m login(token\u001b[38;5;241m=\u001b[39mhf_token)\n",
              "\n",
              "File \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:240\u001b[0m, in \u001b[0;36mDBUtils.SecretsHandler.get\u001b[0;34m(self, scope, key)\u001b[0m\n",
              "\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, scope, key):\n",
              "\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDbutils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msecret\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n",
              "\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n",
              "\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n",
              "\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
              "\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n",
              "\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
              "\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
              "\n",
              "File \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n",
              "\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
              "\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n",
              "\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n",
              "\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
              "\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
              "\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
              "\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
              "\n",
              "\u001b[0;31mIllegalArgumentException\u001b[0m: Secret does not exist with scope: llm_scope and key: huggingface_key"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)\nFile \u001b[0;32m<command-1738672828157799>:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[0;32m----> 3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dbutils\u001b[38;5;241m.\u001b[39msecrets\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_scope\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m hf_token \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m login(token\u001b[38;5;241m=\u001b[39mhf_token)\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:240\u001b[0m, in \u001b[0;36mDBUtils.SecretsHandler.get\u001b[0;34m(self, scope, key)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, scope, key):\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDbutils\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msecret\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\nFile \u001b[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mIllegalArgumentException\u001b[0m: Secret does not exist with scope: llm_scope and key: huggingface_key",
              "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Secret does not exist with scope: llm_scope and key: huggingface_key",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "os.environ[\"huggingface_key\"] = dbutils.secrets.get(\"llm_scope\", \"huggingface_key\")\n",
        "hf_token = os.environ[\"huggingface_key\"]\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c0bf54cc-9bc3-4a43-9771-c30137fe1218",
          "showTitle": false,
          "title": ""
        },
        "id": "UMmInRppFsTt"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "29999630-aae5-45d9-bd21-649349944827",
          "showTitle": false,
          "title": ""
        },
        "id": "U1CIDh4JFsTt"
      },
      "outputs": [],
      "source": [
        "\n",
        "hf_username = <FILL_IN_WITH_YOUR_HUGGINGFACE_USERNAME>\n",
        "peft_model_id = f\"{hf_username}/bloom_prompt_tuning_{time_now}\"\n",
        "trainer.model.push_to_hub(peft_model_id, use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ea9a24b6-871f-40c9-b320-41fd6118cab5",
          "showTitle": false,
          "title": ""
        },
        "id": "iooDw5cuFsTt"
      },
      "source": [
        "### Inference from model in HuggingFace hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ff037802-900f-440c-b0a2-1a388bb86d45",
          "showTitle": false,
          "title": ""
        },
        "id": "NjFJoKrpFsTt",
        "outputId": "ecadf266-ee57-44c0-edf1-aa63693564d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
              "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
              "File \u001b[0;32m<command-1738672828157804>:3\u001b[0m\n",
              "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n",
              "\u001b[0;32m----> 3\u001b[0m config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft_model_id)\n",
              "\u001b[1;32m      4\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft_config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n",
              "\u001b[1;32m      5\u001b[0m peft_random_model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(foundation_model, peft_model_id)\n",
              "\n",
              "\u001b[0;31mNameError\u001b[0m: name 'peft_model_id' is not defined"
            ]
          },
          "metadata": {
            "application/vnd.databricks.v1+output": {
              "arguments": {},
              "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-1738672828157804>:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[0;32m----> 3\u001b[0m config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft_model_id)\n\u001b[1;32m      4\u001b[0m foundation_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft_config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[1;32m      5\u001b[0m peft_random_model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(foundation_model, peft_model_id)\n\n\u001b[0;31mNameError\u001b[0m: name 'peft_model_id' is not defined",
              "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'peft_model_id' is not defined",
              "errorTraceType": "ansi",
              "metadata": {},
              "type": "ipynbError"
            }
          }
        }
      ],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
        "peft_random_model = PeftModel.from_pretrained(foundation_model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "90197941-f912-4a76-af96-783860324c4f",
          "showTitle": false,
          "title": ""
        },
        "id": "5OjjhjJ8FsTt"
      },
      "outputs": [],
      "source": [
        "online_model_outputs = peft_random_model.generate(\n",
        "    input_ids=input1[\"input_ids\"],\n",
        "    attention_mask=input1[\"attention_mask\"],\n",
        "    max_new_tokens=7,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "print(tokenizer.batch_decode(online_model_outputs, skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}